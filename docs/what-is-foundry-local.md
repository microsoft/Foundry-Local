# What is Foundry Local?

Foundry Local is a local version of Azure AI Foundry that enables local execution of large language models (LLMs) directly on your device. This on-device AI inference solution provides privacy, customization, and cost benefits compared to cloud-based alternatives. Best of all, it fits into your existing workflows and applications with an easy-to-use CLI and REST API!

Foundry Local applies the optimization work of ONNX Runtime, Olive, and the ONNX ecosystem, Foundry Local delivers a highly optimized and performant user experience for running AI models locally.

## Key features

- **On-Device Inference**: Run LLMs locally on your own hardware, reducing dependency on cloud services while keeping your data on-device.
- **Model Customization**: Choose from preset models or bring your own to match your specific requirements and use cases.
- **Cost Efficiency**: Avoid recurring cloud service costs by using your existing hardware, making AI tasks more accessible.
- **Seamless Integration**: Easily interface with your applications via an endpoint or test with the CLI, with the option to scale to Azure AI Foundry as your workload demands increase.

## Use cases

Foundry Local is ideal for scenarios where:

- Data privacy and security are paramount
- You need to operate in environments with limited or no internet connectivity
- You want to reduce cloud inference costs
- You need low-latency AI responses for real-time applications
- You want to experiment with AI models before deploying to a cloud environment

## Pricing and billing

Entirely Free! You're using your own hardware, and there are no extra costs associated with running AI models locally.

## How to get access

Download from the Microsoft Store. (WIP)

## Next steps

- [Get started with Foundry Local](./get-started.md)
- [Compile Hugging Face models for Foundry Local](./how-to/compile-models-for-foundry-local.md)
- [Learn more about ONNX Runtime](https://onnxruntime.ai/docs/)
